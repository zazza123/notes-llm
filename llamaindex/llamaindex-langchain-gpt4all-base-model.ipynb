{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **LlamaIndex** combined with **LangChain** to use GPT4All Model\n",
    "\n",
    "*This notebook was created on **22/07/2024** using python 3.12 version.*\n",
    "\n",
    "In this notebook we will use `llama_index` library combined with `langchain` to load a **GPT4All** model on your computer, and use RUG to ask question to the model on topics that where not part of the training set of the model itself.\n",
    "\n",
    "For this example, we are going to use the model **SBert** (*all-MiniLM-L6-v2.gguf2.f16.gguf*) as embedding model, and model **GPT4All Falcon** (*gpt4all-falcon-newbpe-q4_0.gguf*) for querying our documents. \\\n",
    "Observe that for this model is suggested a 8 GB RAM computer.\n",
    "\n",
    "##### Remark\n",
    "\n",
    "Before running this tutorial is suggested to read the previous Jupyter Notebooks:\n",
    "\n",
    "1. gpt4all/gpt4all-model-list.ipynb\n",
    "2. gpt4all/gpt4all-base-model.ipynb\n",
    "3. langchain/langchain-gpt4all-base-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install gpt4all==2.7.0\n",
    "%pip install llama-index==0.10.56\n",
    "%pip install llama-index-llms-langchain==0.1.4\n",
    "%pip install llama-index-embeddings-langchain==0.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from pathlib import Path\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.llms.langchain import LangChainLLM\n",
    "from langchain_community.llms.gpt4all import GPT4All\n",
    "\n",
    "from gpt4all import Embed4All\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain_community.embeddings.gpt4all import GPT4AllEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input text\n",
    "documents = SimpleDirectoryReader(\"../data\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model path and name\n",
    "model_path = Path().home() / \".data\" / \"model\"\n",
    "if not model_path.exists():\n",
    "    raise Exception(f\"The path '{model_path}' does not exists!\")\n",
    "\n",
    "embedding_model_name = \"all-MiniLM-L6-v2.gguf2.f16.gguf\"\n",
    "embedding_model_path_name = model_path / embedding_model_name\n",
    "\n",
    "model_name = \"gpt4all-falcon-newbpe-q4_0.gguf\"\n",
    "model_path_name = model_path / model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load gpt4all model\n",
    "model = GPT4All(model = str(model_path_name))\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM created!\n"
     ]
    }
   ],
   "source": [
    "# Create LLM using Llama Index\n",
    "llm = LangChainLLM(llm = model)\n",
    "print(\"LLM created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 1/1 [00:00<00:00, 441.13it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:00<00:00,  3.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Document Index\n",
    "gpt4all_embedding = GPT4AllEmbeddings(client = Embed4All(model_name = str(embedding_model_path_name)))\n",
    "embed_model = LangchainEmbedding(langchain_embeddings = gpt4all_embedding)\n",
    "index = VectorStoreIndex.from_documents(documents, embed_model = embed_model, show_progress = True)\n",
    "print(\"Index created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query Engine created!\n"
     ]
    }
   ],
   "source": [
    "# Create Query Engine\n",
    "query_engine = index.as_query_engine(llm = llm)\n",
    "print(\"Query Engine created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Lydia and Andrew are going deep into the Forbidden Forest to find a hidden creature that could be incredibly important for Andrew's research.\"\n"
     ]
    }
   ],
   "source": [
    "# Ask question\n",
    "response = query_engine.query(\"Where are Lydia and Andrew going during the night?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312gpt4all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
