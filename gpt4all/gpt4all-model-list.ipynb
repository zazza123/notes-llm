{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT4All (Models)\n",
    "\n",
    "This notebook was created on **22/07/2024** using python 3.12 version.\n",
    "\n",
    "The notebook is used to provide the list of available GPT4All models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gpt4all in /Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: requests in /Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages (from gpt4all) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages (from gpt4all) (4.66.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages (from requests->gpt4all) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages (from requests->gpt4all) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages (from requests->gpt4all) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andreazanini/.pyenv/versions/3.12.0/envs/py312gpt4all/lib/python3.12/site-packages (from requests->gpt4all) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install all required libraries\n",
    "%pip install gpt4all==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt4all import GPT4All\n",
    "\n",
    "models = GPT4All.list_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama 3 Instruct\n",
      " - filename    : Meta-Llama-3-8B-Instruct.Q4_0.gguf\n",
      " - filesize    : 4.34 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 8 billion\n",
      " - type        : LLaMA3\n",
      " - url         : https://gpt4all.io/models/gguf/Meta-Llama-3-8B-Instruct.Q4_0.gguf\n",
      " - description : <ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in Llama 3 format</li><li>Trained by Meta</li><li>License: <a href=\"https://llama.meta.com/llama3/license/\">Meta Llama 3 Community License</a></li></ul>\n",
      "Nous Hermes 2 Mistral DPO\n",
      " - filename    : Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\n",
      " - filesize    : 3.83 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : Mistral\n",
      " - url         : https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO-GGUF/resolve/main/Nous-Hermes-2-Mistral-7B-DPO.Q4_0.gguf\n",
      " - description : <strong>Best overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Accepts system prompts in ChatML format</li><li>Trained by Mistral AI<li>Finetuned by Nous Research on the OpenHermes-2.5 dataset<li>Licensed for commercial use</ul>\n",
      "Mistral Instruct\n",
      " - filename    : mistral-7b-instruct-v0.1.Q4_0.gguf\n",
      " - filesize    : 3.83 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : Mistral\n",
      " - url         : https://gpt4all.io/models/gguf/mistral-7b-instruct-v0.1.Q4_0.gguf\n",
      " - description : <strong>Strong overall fast instruction following model</strong><br><ul><li>Fast responses</li><li>Trained by Mistral AI<li>Uncensored</li><li>Licensed for commercial use</li></ul>\n",
      "Mistral OpenOrca\n",
      " - filename    : mistral-7b-openorca.gguf2.Q4_0.gguf\n",
      " - filesize    : 3.83 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : Mistral\n",
      " - url         : https://gpt4all.io/models/gguf/mistral-7b-openorca.gguf2.Q4_0.gguf\n",
      " - description : <strong>Strong overall fast chat model</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by Mistral AI<li>Finetuned on OpenOrca dataset curated via <a href=\"https://atlas.nomic.ai/\">Nomic Atlas</a><li>Licensed for commercial use</ul>\n",
      "GPT4All Falcon\n",
      " - filename    : gpt4all-falcon-newbpe-q4_0.gguf\n",
      " - filesize    : 3.92 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : Falcon\n",
      " - url         : https://gpt4all.io/models/gguf/gpt4all-falcon-newbpe-q4_0.gguf\n",
      " - description : <strong>Very fast model with good quality</strong><br><ul><li>Fastest responses</li><li>Instruction based</li><li>Trained by TII<li>Finetuned by Nomic AI<li>Licensed for commercial use</ul>\n",
      "Orca 2 (Medium)\n",
      " - filename    : orca-2-7b.Q4_0.gguf\n",
      " - filesize    : 3.56 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : LLaMA2\n",
      " - url         : https://gpt4all.io/models/gguf/orca-2-7b.Q4_0.gguf\n",
      " - description : <ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>\n",
      "Orca 2 (Full)\n",
      " - filename    : orca-2-13b.Q4_0.gguf\n",
      " - filesize    : 6.86 GB\n",
      " - RAM         : 16 GB\n",
      " - parameters  : 13 billion\n",
      " - type        : LLaMA2\n",
      " - url         : https://gpt4all.io/models/gguf/orca-2-13b.Q4_0.gguf\n",
      " - description : <ul><li>Instruction based<li>Trained by Microsoft<li>Cannot be used commercially</ul>\n",
      "Wizard v1.2\n",
      " - filename    : wizardlm-13b-v1.2.Q4_0.gguf\n",
      " - filesize    : 6.86 GB\n",
      " - RAM         : 16 GB\n",
      " - parameters  : 13 billion\n",
      " - type        : LLaMA2\n",
      " - url         : https://gpt4all.io/models/gguf/wizardlm-13b-v1.2.Q4_0.gguf\n",
      " - description : <strong>Strong overall larger model</strong><br><ul><li>Instruction based<li>Gives very long responses<li>Finetuned with only 1k of high-quality data<li>Trained by Microsoft and Peking University<li>Cannot be used commercially</ul>\n",
      "Ghost 7B v0.9.1\n",
      " - filename    : ghost-7b-v0.9.1-Q4_0.gguf\n",
      " - filesize    : 3.83 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : Mistral\n",
      " - url         : https://huggingface.co/lamhieu/ghost-7b-v0.9.1-gguf/resolve/main/ghost-7b-v0.9.1-Q4_0.gguf\n",
      " - description : <strong>Ghost 7B v0.9.1</strong> fast, powerful and smooth for Vietnamese and English languages.\n",
      "Hermes\n",
      " - filename    : nous-hermes-llama2-13b.Q4_0.gguf\n",
      " - filesize    : 6.86 GB\n",
      " - RAM         : 16 GB\n",
      " - parameters  : 13 billion\n",
      " - type        : LLaMA2\n",
      " - url         : https://gpt4all.io/models/gguf/nous-hermes-llama2-13b.Q4_0.gguf\n",
      " - description : <strong>Extremely good model</strong><br><ul><li>Instruction based<li>Gives long responses<li>Curated with 300,000 uncensored instructions<li>Trained by Nous Research<li>Cannot be used commercially</ul>\n",
      "Snoozy\n",
      " - filename    : gpt4all-13b-snoozy-q4_0.gguf\n",
      " - filesize    : 6.86 GB\n",
      " - RAM         : 16 GB\n",
      " - parameters  : 13 billion\n",
      " - type        : LLaMA\n",
      " - url         : https://gpt4all.io/models/gguf/gpt4all-13b-snoozy-q4_0.gguf\n",
      " - description : <strong>Very good overall model</strong><br><ul><li>Instruction based<li>Based on the same dataset as Groovy<li>Slower than Groovy, with higher quality responses<li>Trained by Nomic AI<li>Cannot be used commercially</ul>\n",
      "MPT Chat\n",
      " - filename    : mpt-7b-chat-newbpe-q4_0.gguf\n",
      " - filesize    : 3.64 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : MPT\n",
      " - url         : https://gpt4all.io/models/gguf/mpt-7b-chat-newbpe-q4_0.gguf\n",
      " - description : <strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>\n",
      "MPT Chat\n",
      " - filename    : mpt-7b-chat.gguf4.Q4_0.gguf\n",
      " - filesize    : 3.54 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : MPT\n",
      " - url         : https://gpt4all.io/models/gguf/mpt-7b-chat.gguf4.Q4_0.gguf\n",
      " - description : <strong>Good model with novel architecture</strong><br><ul><li>Fast responses<li>Chat based<li>Trained by Mosaic ML<li>Cannot be used commercially</ul>\n",
      "Phi-3 Mini Instruct\n",
      " - filename    : Phi-3-mini-4k-instruct.Q4_0.gguf\n",
      " - filesize    : 2.03 GB\n",
      " - RAM         : 4 GB\n",
      " - parameters  : 4 billion\n",
      " - type        : Phi-3\n",
      " - url         : https://gpt4all.io/models/gguf/Phi-3-mini-4k-instruct.Q4_0.gguf\n",
      " - description : <ul><li>Very fast responses</li><li>Chat based model</li><li>Accepts system prompts in Phi-3 format</li><li>Trained by Microsoft</li><li>License: <a href=\"https://opensource.org/license/mit\">MIT</a></li><li>No restrictions on commercial use</li></ul>\n",
      "Mini Orca (Small)\n",
      " - filename    : orca-mini-3b-gguf2-q4_0.gguf\n",
      " - filesize    : 1.84 GB\n",
      " - RAM         : 4 GB\n",
      " - parameters  : 3 billion\n",
      " - type        : OpenLLaMa\n",
      " - url         : https://gpt4all.io/models/gguf/orca-mini-3b-gguf2-q4_0.gguf\n",
      " - description : <strong>Small version of new model with novel dataset</strong><br><ul><li>Very fast responses</li><li>Instruction based</li><li>Explain tuned datasets</li><li>Orca Research Paper dataset construction approaches</li><li>Cannot be used commercially</li></ul>\n",
      "Replit\n",
      " - filename    : replit-code-v1_5-3b-newbpe-q4_0.gguf\n",
      " - filesize    : 1.82 GB\n",
      " - RAM         : 4 GB\n",
      " - parameters  : 3 billion\n",
      " - type        : Replit\n",
      " - url         : https://gpt4all.io/models/gguf/replit-code-v1_5-3b-newbpe-q4_0.gguf\n",
      " - description : <strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>Licensed for commercial use<li>WARNING: Not available for chat GUI</ul>\n",
      "Starcoder\n",
      " - filename    : starcoder-newbpe-q4_0.gguf\n",
      " - filesize    : 8.37 GB\n",
      " - RAM         : 4 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : Starcoder\n",
      " - url         : https://gpt4all.io/models/gguf/starcoder-newbpe-q4_0.gguf\n",
      " - description : <strong>Trained on subset of the Stack</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</ul>\n",
      "Rift coder\n",
      " - filename    : rift-coder-v0-7b-q4_0.gguf\n",
      " - filesize    : 3.56 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : LLaMA\n",
      " - url         : https://gpt4all.io/models/gguf/rift-coder-v0-7b-q4_0.gguf\n",
      " - description : <strong>Trained on collection of Python and TypeScript</strong><br><ul><li>Code completion based<li>WARNING: Not available for chat GUI</li>\n",
      "SBert\n",
      " - filename    : all-MiniLM-L6-v2-f16.gguf\n",
      " - filesize    : 0.04 GB\n",
      " - RAM         : 1 GB\n",
      " - parameters  : 40 million\n",
      " - type        : Bert\n",
      " - url         : https://gpt4all.io/models/gguf/all-MiniLM-L6-v2-f16.gguf\n",
      " - description : <strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)\n",
      "SBert\n",
      " - filename    : all-MiniLM-L6-v2.gguf2.f16.gguf\n",
      " - filesize    : 0.04 GB\n",
      " - RAM         : 1 GB\n",
      " - parameters  : 40 million\n",
      " - type        : Bert\n",
      " - url         : https://gpt4all.io/models/gguf/all-MiniLM-L6-v2.gguf2.f16.gguf\n",
      " - description : <strong>LocalDocs text embeddings model</strong><br><ul><li>For use with LocalDocs feature<li>Used for retrieval augmented generation (RAG)\n",
      "EM German Mistral\n",
      " - filename    : em_german_mistral_v01.Q4_0.gguf\n",
      " - filesize    : 3.83 GB\n",
      " - RAM         : 8 GB\n",
      " - parameters  : 7 billion\n",
      " - type        : Mistral\n",
      " - url         : https://huggingface.co/TheBloke/em_german_mistral_v01-GGUF/resolve/main/em_german_mistral_v01.Q4_0.gguf\n",
      " - description : <strong>Mistral-based model for German-language applications</strong><br><ul><li>Fast responses</li><li>Chat based model</li><li>Trained by ellamind<li>Finetuned on German instruction and chat data</a><li>Licensed for commercial use</ul>\n",
      "Nomic Embed Text v1\n",
      " - filename    : nomic-embed-text-v1.f16.gguf\n",
      " - filesize    : 0.26 GB\n",
      " - RAM         : 1 GB\n",
      " - parameters  : 137 million\n",
      " - type        : Bert\n",
      " - url         : https://gpt4all.io/models/gguf/nomic-embed-text-v1.f16.gguf\n",
      " - description : nomic-embed-text-v1\n",
      "Nomic Embed Text v1.5\n",
      " - filename    : nomic-embed-text-v1.5.f16.gguf\n",
      " - filesize    : 0.26 GB\n",
      " - RAM         : 1 GB\n",
      " - parameters  : 137 million\n",
      " - type        : Bert\n",
      " - url         : https://gpt4all.io/models/gguf/nomic-embed-text-v1.5.f16.gguf\n",
      " - description : nomic-embed-text-v1.5\n"
     ]
    }
   ],
   "source": [
    "for model in models:\n",
    "    print(model[\"name\"])\n",
    "    print(\" - filename\".ljust(15) + f\": {model['filename']}\")\n",
    "    print(\" - filesize\".ljust(15) + f\": {round(int(model['filesize'])/1024**3, 2)} GB\")\n",
    "    print(\" - RAM\".ljust(15) + f\": {model['ramrequired']} GB\")\n",
    "    print(\" - parameters\".ljust(15) + f\": {model['parameters']}\")\n",
    "    print(\" - type\".ljust(15) + f\": {model['type']}\")\n",
    "    print(\" - url\".ljust(15) + f\": {model['url']}\")\n",
    "    print(\" - description\".ljust(15) + f\": {model['description']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312gpt4all",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
